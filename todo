todo

inference with detector + maximal bbox
- online or offline? online

test mvd:
- [x] download ann for gt video.
- [x] make inference mvd {frame: probs}
- [ ] make inference with detector (maximal bbox):
    Deploy custom rtdetr-v2 in separate docker -> session.inference_image_batch(frames)
- [ ] visualize Positive fragments (use Claude chat)
- [ ] benchmark: inference -> sly_ann tags -> benchmark format
- [ ] gt ann -> benchmark format
- [ ] run benchmark

task:
1. clone rt-detr. requirements.txt? the same docker? torch version?
2. local code or fastapi server?
3. inference code
4. benchmark

speed up:
- use cv2 video_reader to read video faster
- multiprocessing reader
- may be use local detector instead of in a separate docker